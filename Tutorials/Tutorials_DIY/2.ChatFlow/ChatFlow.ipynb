{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2\n",
    "\n",
    "**In this tutorial you will:**\n",
    "- learn how to pull a flow from the FlowVerse [Section 1](#1-flowverse)\n",
    "- Familiarized yourself with a curcial the ChatAtomicFlow: [Section 2](#2-chatatomicflow)\n",
    "- Learn how to customize existing flows to your needs [Section 2](#231-customizing-chatatomicflow-creating-a-chatbot-that-answers-in-a-given-language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#imports\n",
    "from aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n",
    "from aiflows.utils import serving\n",
    "from aiflows.utils import colink_utils\n",
    "from aiflows.workers import run_dispatch_worker_thread\n",
    "from aiflows.base_flows import AtomicFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from aiflows import flow_verse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils import compile_and_writefile, dict_to_yaml\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a local colink server\n",
    "cl = colink_utils.start_colink_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Worker thread\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequesite:\n",
    "\n",
    "- You should have an Openai API key. Check out [this link](https://platform.openai.com/docs/quickstart/account-setup) to set up an account an get one. Then set your key as an environment variable. For example, with conda:\n",
    "    ```unix\n",
    "    conda env config vars set OPENAI_API_KEY=<YOUR-KEY>\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FlowVerse\n",
    "\n",
    "### 1.1 What's the FlowVerse ? \n",
    "The FlowVerse is the hub of flows created and shared by our amazing community for everyone to use! These flows are shared on Hugging Face with the intention of being reused by others. Explore our Flows on the FlowVerse [here](https://huggingface.co/aiflows)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pulling a Flow from the FlowVerse\n",
    "\n",
    "To pull the `ChatFlowModule` (check out its card [here](https://huggingface.co/aiflows/ChatFlowModule)) from the FlowVerse, we need to use the `flow_verse.sync_dependencies` function. This function will pull the flow into the current directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aiflows import flow_verse\n",
    "# ~~~ Load Flow dependecies from FlowVerse ~~~\n",
    "dependencies = [\n",
    "    {\"url\": \"aiflows/ChatFlowModule\", \"revision\": \"main\"},\n",
    "]\n",
    "\n",
    "flow_verse.sync_dependencies(dependencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this down:\n",
    "* `dependencies` is a list of dictionaries (in this case, there's only one) indicating which FlowModules we want to pull from the FlowVerse. The dictionary contains two key-value pairs:\n",
    "  * `url`: Specifies the URL where the flow can be found on Hugging Face. Here, the URL is `aiflows/ChatFlowModule`, where `aiflows` is the name of our organization on Hugging Face (or the username of a user hosting their flow on Hugging Face), and `ChatFlowModule` is the name of the FlowModule containing the `ChatAtomicFlow` on the FlowVerse. Note that the `url` is literally the address of the `ChatFlowModule` on Hugging Face (excluding the https://huggingface.co/). So if you type https://huggingface.co/aiflows/ChatFlowModule in your browser, you will find the Flow.\n",
    "  * `revision`: Represents the revision id (i.e., the full commit hash) of the commit we want to fetch. Note that if you set `revision` to `main`, it will fetch the latest commit on the main branch.\n",
    "\n",
    "Note that running the cell above will pull the flow in **flow_modules/aiflows/ChatFlowModule**.\n",
    "\n",
    "Now that we've fetched the `ChatAtomicFlowModule` from the FlowVerse, we can start creating our own personalized Flow from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChatAtomicFlow\n",
    "\n",
    "The ChatAtomicFlow is a minimal wrapper for querying an LLM via an API to generate textuals responses to textual inputs. It employs litellm as a backend to query the LLM via an API. See litellm's supported models and APIs here: https://docs.litellm.ai/docs/providers. In this tutorial, we will be using `openai` as the provider. But you can use any provider supported by litellm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inspecting ChatAtomicFlow's Default Configuration\n",
    "\n",
    "Let's start by inspecting the default configuration of the `ChatAtomicFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the default configuration\n",
    "default_cfg = read_yaml_file(\"flow_modules/aiflows/ChatFlowModule/ChatAtomicFlow.yaml\")\n",
    "print(json.dumps(default_cfg, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full list of parameters and their descriptions, please refer to the flow's card [here](https://huggingface.co/aiflows/ChatFlowModule#chatatomicflow-objects) (under configuration parameters). We will be discussing the most important parameters in this tutorial:\n",
    "\n",
    "- **`input_interface_initialized` and `input_interface_non_initialized`**: These parameters in our configuration specify the keys expected in the input data dictionary when the `ChatAtomicFlow` is called for the first time and subsequently. \n",
    "  - `input_interface_non_initialized`: Specifies the keys expected in the input data dictionary when the `ChatAtomicFlow` is called for the first time the flow is called.\n",
    "  - `input_interface_initialized`: Specifies the keys expected in the input data dictionary after the first call, essentially serving a role similar to the regular `input_interface`. \n",
    "The distinction between the two becomes apparent when different inputs are required for the initial query compared to subsequent queries. \n",
    "For instance, in ReAct, the first time you query the LLM, the input is provided by a human, such as a question. In subsequent queries, the input comes from the execution of a tool (e.g., a query to Wikipedia). In ReAct's case, these two scenarios are distinguished by `ChatAtomicFlow`'s `input_interface_non_initialized` and `input_interface_initialized` parameters.\n",
    "\n",
    "-  `backend` is a dictionary containing parameters specific to the LLM. These parameters include:\n",
    "    - `api_infos`: Your API information (which will be passed later for privacy reasons).\n",
    "    - `model_name`: A dictionary with key-item pairs, where keys correspond to the `backend_used` attribute of the `ApiInfo` class for the chosen backend, and values represent the desired model for that backend. Model selection depends on the provided `api_infos`. Additional models can be added for different backends, following LiteLLM's naming conventions (refer to LiteLLM's supported providers and model names [here](https://docs.litellm.ai/docs/providers)). For instance, with an Anthropic API key and a desire to use \"claude-2,\" one would check Anthropic's model details [here](https://docs.litellm.ai/docs/providers/anthropic#model-details). As \"claude-2\" is named the same in LiteLLM, the `model_name` dictionary would be updated as follows:\n",
    "      ```yaml\n",
    "      backend:\n",
    "      _target_: aiflows.backends.llm_lite.LiteLLMBackend\n",
    "      api_infos: ???\n",
    "      model_name:\n",
    "        openai: \"gpt-3.5-turbo\"\n",
    "        anthropic: \"claude-2\"\n",
    "      ```\n",
    "    - `n`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty` are generation parameters for LiteLLM's completion function (refer to all possible generation parameters [here](https://docs.litellm.ai/docs/completion/input#input-params-1)).\n",
    "\n",
    "- `system_message_prompt_template`: This is the system prompt template passed to the LLM.\n",
    "- `init_human_message_prompt_template`: This is the user prompt template passed to the LLM the first time the flow is called.\n",
    "- `human_message_prompt_template`: This is the user prompt template passed to the LLM after the first time the flow is called.\n",
    "\n",
    "All 3 prompts are in Jinja format and contain the following configurable parameter:\n",
    "  - `template`: The prompt template in Jinja format.\n",
    "  - `input_variables`: The input variables of the prompt to be passed to the Jinja template (passed when the flow is called).\n",
    "  - `partial_variables`: The partial variables of the prompt to be passed to the Jinja template (passed directly from the config).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Serving the default Configuration\n",
    "\n",
    "Now that we have a basic understanding of the `ChatAtomicFlow`'s configuration, let's serve the default configuration of the `ChatAtomicFlow` by calling the `serve` method. This method will return the default configuration of the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow\",\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Customizing ChatAtomicFlow: Creating a Chatbot that Answers in a given Language\n",
    "\n",
    "To showcase how to customize the `ChatAtomicFlow`, we will create a chatbot that answers in a given language. We will use the `ChatAtomicFlow` to create a chatbot that answers in French. The key here is to understand how to get an instance of the `ChatAtomicFlow` with your desired configuration overriding the default configuration.\n",
    "\n",
    "To do this, we will:\n",
    "- 1. Copy the default configuration of the `ChatAtomicFlow`\n",
    "- 2. Override the `system_message_prompt_template` to include the language we want the answer in\n",
    "- 3. Load our API key in the `api_infos` field of the configuration\n",
    "- 4. Get an instance of the `ChatAtomicFlow` with our desired configuration overrides\n",
    "- 5. Run the `ChatAtomicFlow` with our desired configuration !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Copy the default configuration of the `ChatAtomicFlow`\n",
    "\n",
    "chatbot_overrides = copy.deepcopy(default_cfg)\n",
    "\n",
    "# STEP 2: Override the system_message_prompt_template to prompt the user to speak in French\n",
    "language = \"French\"\n",
    "###### TODO: Write a system prompt Which constrains the model to reply in french (no need to for it be long)\n",
    "chatbot_overrides[\"system_message_prompt_template\"][\"template\"] = ??? \n",
    "\n",
    "# STEP 3: Load the API keys\n",
    "from aiflows.backends.api_info import ApiInfo\n",
    "\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "#load api keys to goncig\n",
    "quick_load_api_keys(chatbot_overrides, api_info)\n",
    "\n",
    "#STEP 4: Get an instance of the `ChatAtomicFlow` with the overrides\n",
    "french_chatbot = serving.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    "    config_overrides=chatbot_overrides,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Send messages to the chatbot\n",
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = french_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Customizing ChatAtomicFlow: Changing the model\n",
    "\n",
    "Now we could also change model to use a different language model. For example, we could use the `gpt-4` model to answer in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Copy the default configuration of the `ChatAtomicFlow`\n",
    "english_chatbot_gpt4_overrides = copy.deepcopy(default_cfg)\n",
    "\n",
    "# STEP 2: ##### TODO: Override the model_name to use GPT-4 ######\n",
    "???\n",
    "\n",
    "# STEP 3: Load the API keys\n",
    "api_info = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(english_chatbot_gpt4_overrides, api_info)\n",
    "\n",
    "#STEP 4: Get an instance of the `ChatAtomicFlow` with the overrides\n",
    "english_chatbot_gpt4 = serving.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatAtomicFlow\",\n",
    "    ??? ### TODO: override the default configuration of ChatAtomicFlow with your own\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Send messages to the chatbot\n",
    "\n",
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"What is the capital of Switzerland?\"},\n",
    ")\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"Where is it located?\"},\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = english_chatbot_gpt4.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Customizing ChatAtomicFlow: Creating an instruction-based chatbot\n",
    "\n",
    "Now let's write a chatbot that completes your messages by substituting whatever you write in double brackets \"[[]]\" with the instructions you give it. For example, \"the capital of Switzerland is [[insert capital]]\" should return \"the capital of Switzerland is Bern\". This is a simple example of a chatbot that could augment your writing by providing you with the information you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepcopy of demo config\n",
    "overrides_config = copy.deepcopy(default_cfg)\n",
    "\n",
    "#TODO: Mount a Chatflow who generates text with a `temperature = 1` and a `system_message_prompt_template` that is personalized\n",
    "???\n",
    "\n",
    "#loading api key to config\n",
    "???\n",
    "\n",
    "\n",
    "#TO DO get a flow instance\n",
    "personalized_chatbot = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message1 = FlowMessage(\n",
    "    data={\"id\": 0, \"query\": \"The capital of Switzerland is [[insert capital]].\"},\n",
    ")\n",
    "\n",
    "query_2 = \\\n",
    "\"\"\"\n",
    "Dear Margaret,\n",
    "[[Ask about her day and talk about the sunny weather]]\n",
    "\n",
    "[[Ask if she has finished her part of the report]]. I've finished my part and I'm looking forward to the weekend!\n",
    "\n",
    "[[Sign of with my name: Nicolas Baldwin]].\n",
    "\"\"\"\n",
    "\n",
    "input_message2 = FlowMessage(\n",
    "    data={\n",
    "        \"id\": 0,\n",
    "        \"query\": query_2\n",
    "    },\n",
    ")\n",
    "\n",
    "messages = [input_message1, input_message2]\n",
    "\n",
    "for msg in messages:\n",
    "    print(\"~~~~Sent message~~~~\\n\", msg.data)\n",
    "    future = personalized_chatbot.get_reply_future(msg)\n",
    "    reply = future.get_data()\n",
    "    print(\"~~~~Reply~~~~ \\n\",reply[\"api_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
